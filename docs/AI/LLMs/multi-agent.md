# Multi-Agent
先聊一下传统意义上的 Agent，说清楚 LLM 引入 Agent 的背景和 motivation，再介绍一种对其理论建模的方式，以及比较强大的几款方便落地的框架，尤其是 AutoGen，最后谈谈潜在的一些典型问题。
### Classic Agent
Agent 不是一个新鲜的概念，在 LLM 引起广泛注意之前已经有了很多应用，实际上，它是对一个「有一定自主意识可以独立采取行动的代理人」的抽象。

经济或者商业领域有「代理人」和「委托人」的关系，政治或者国际关系上也有可以「机动行事」的外交官或特使，计算机特别是 AI 领域里，Agent 则指代可以感知其环境并根据该环境自主行动以达成其目标的系统或软件实体，其可以是简单的软件程序，也可以是复杂的机器人。

沿着这个路径，也有一些在不同行业里对 Agent 进行建模的理论。

代理理论（Principal-Agent Theory）主要用于经济学和管理学，探讨当一个人（代理人）代表另一个人（委托人）行事时可能出现的问题。核心问题是信息不对称和利益冲突。数学上，通常通过构建一个最优化模型来处理，其中，委托人设计一个激励机制（如工资、奖金等），以最大化其自身的效用，同时也考虑到代理人的行为反应。

博弈论是研究具有冲突和合作动机的理性决策者（Agent）之间互动的数学理论，提供了一系列模型，用于分析多个 Agent 在特定规则下的策略选择。其中，博弈可以是合作的或非合作的，信息可以是完全的或不完全的。

- **纳什均衡**：在非合作博弈中，纳什均衡是每个玩家选择策略的一种状态，且无一玩家能通过改变策略单独获得更好的结果。
- **重复博弈与演化博弈**：考虑在多次互动中策略如何演化。

决策理论涉及代理人在不确定性下如何做出最优选择的研究，通常涉及到概率论和统计决策理论。

- **期望效用理论**：代理人根据其效用函数和各种行动结果的概率加权平均来选择行动。
- **贝叶斯决策理论**：在信息不完全的情况下，代理人如何利用先验知识和观测数据更新其信念，并做出决策。
#### MDP

计算机科学中的 Agent 模型通常通过算法和计算方法来实现，例如**马尔可夫决策过程（Markov decision process, MDP）**，用于模拟在不确定环境中的决策过程。Agent 基于当前状态和可能的转移概率来选择行动，以最大化长期回报。

具体地，MDP 是一种数学框架，用于建模决策制定过程，其结果依赖于决策者的行为，并且部分依赖于随机因素。MDP 是强化学习和其他多种算法的基础，特别适用于那些可以通过状态、行动和奖励来描述的问题。强化学习是一种机器学习方法，其中一个 Agent 通过与环境交互来学习如何最大化某种累积奖励，使用 MDP 作为其理论基础，但强调的是在没有完整环境模型的情况下学习最优策略。

MDP 的组成元素：

1. **状态空间（S）**：代表所有可能的环境状态。
2. **行动空间（A）**：在给定状态下，Agent 可以选择的所有可能行动。
3. **转移概率（P）**：$P(s'|s, a)$ 表示在状态 $s$ 下采取行动 $a$ 后转移到状态 $s'$ 的概率。
4. **奖励函数（R）**：$R(s, a)$ 表示在状态 $s$ 下采取行动 $a$ 所获得的即时奖励。

MDP 的目标是找到一个策略 $\pi$，为每个状态 $s$ 指定一个行动 $a$，以最大化预期的累积奖励，通常是折扣奖励的总和，即：
$$
V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R(s_t, a_t) \mid s_0 = s, \pi\right]
$$
其中，$\gamma$ 是折扣因子，介于 0 和 1 之间，用于调整未来奖励的当前价值。$V^\pi(s)$ 表示在给定策略 $\pi$ 下，从状态 $s$ 开始的预期累积折扣奖励。我们逐项解析这个式子，以便更好地理解它的数学含义和实际意义：

1. **$s_0 = s$**：表示考虑的起始状态是 $s$。我们想要评估从这个特定状态开始，遵循策略 $\pi$ 时的长期表现。

2. **策略 $\pi$**：是一个从状态到行动的映射。对于每一个状态 $s$，策略 $\pi$ 指定一个行动 $a$，即 $\pi(s) = a$，$\pi$ 决定了在每个状态下应选择哪个行动。

3. **$\sum_{t=0}^\infty \gamma^t R(s_t, a_t)$**：
	- **求和符号 $\sum$**：表示考虑所有从时间 $t = 0$ 到无穷的奖励。
	- **折扣因子 $\gamma^t$**：一个介于 0 和 1 之间的数，表示奖励随时间的衰减。折扣因子的作用是使得远期奖励的现值减小，有助于奖励总和的收敛，并且反映了未来不确定性的影响。
	- **奖励 $R(s_t, a_t)$**：在时间 $t$，状态为 $s_t$，并采取行动 $a_t$ 时获得的即时奖励。

4. **期望 \( \mathbb{E} \)**：表示正在计算的是期望值，即平均意义上的累积折扣奖励。期望是考虑到所有可能的未来状态序列，根据转移概率和策略 $\pi$ 的概率分布来计算的。

将这些组件结合起来，$V^\pi(s)$ 表示在策略 $\pi$ 的指导下，从状态 $s$ 开始，期望获得的所有未来奖励的折扣总和的平均值。这个值提供了一个量化的度量，表明遵循策略 $\pi$ 时，从状态 $s$ 开始的长期效用是多少。通过计算和比较不同策略下的 $V^\pi(s)$ 值，可以评估哪个策略在长期内更有效，从而帮助选择或优化策略。

**总体上，一个良好的 Agent 需要可以在复杂、不确定环境中进行有效决策。**
## Motivation

单个 LLM 在处理复杂任务时已经显示出不可小觑的能力，但在一些复杂问题上，单个 LLM 依旧力不从心。此时一个自然的考虑就是将问题做 modularization，让多个 LLM 分别去解决多个子问题，如此，即可将一个复杂问题分解为多个子任务，分别处理。宏观来看，解决单个子问题的单个 LLM 实际上就是一个 Agent（自主决策）。

而传统意义上的 Agent 在 LLM 语境下并不完全适用——LLM 语境中的 Agent 不仅能够独立作出决策和响应，还必须能够与其他 Agent 进行复杂的交互对话，这在传统的 Agent 设计中不是必须的，所以，LLM 语境下的 Agent 相对来说被赋予了更具体的定义和功能，特别是强调了多智能体之间的交流和协作能力。

例如，AutoGen 中的 Agent 是可以发送消息、接收消息并通过模型、工具或人类输入（或它们的组合）生成回复的实体。这种设计使得 Agent 可以模拟现实世界中的实体（如人类、算法等），并简化了作为智能体协作的复杂工作流的实现。
## Principle

> TODO 
#### Construction

##### Profiling Module

##### Memory Module

##### Planning Module

##### Action Module

#### Capability Acquisition

##### With Fine-tuning

##### Without Fine-tuning

## Implementation
#### AutoGen
## More

1. [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432)

	Submitted on 22 Aug 2023 (v 1), last revised 4 Apr 2024 (this version, v 5)

2. [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864)

	Submitted on 14 Sep 2023 (v 1), last revised 19 Sep 2023 (this version, v 3)

3. [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)

	Submitted on 16 Aug 2023 ([v1](https://arxiv.org/abs/2308.08155v1)), last revised 3 Oct 2023 (this version, v 2)

4. [https://microsoft.github.io/autogen/](https://microsoft.github.io/autogen/)